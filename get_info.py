import requests
import re
from bs4 import BeautifulSoup


def scrape_urls(urls, output_file):
    with open(output_file, 'w', encoding='utf-8') as md_file:
        print("Generating Report")
        md_file.write('\n## VULNERABILITIES REPORT \n')
        for url in urls:
            print(f"scrapping {url}")
            try:
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                title_tag = soup.find('h1')
                title = title_tag.text.strip() if title_tag else 'No title found'
                md_file.write(f'# {title} \n')
                container = soup.find('div', class_='css-1ooohex-HTML-container-body')

                if container:
                    paragraphs = container.find_all('p')
                else:
                    paragraphs = []
                if not paragraphs:
                    md_file.write(f'No data found.\n\n')
                else:
                    for paragraph in paragraphs:
                        md_file.write(f'{paragraph.text}\n\n')

            except requests.RequestException as e:
                md_file.write(f'### {url}\n')
                md_file.write(f'Something failed: {e}\n\n')


def get_url():
    try:
        with open('found_vulnerabilities.md', 'r') as file:
            lines = file.readlines()

        url_pattern = re.compile(r'https?://\S+')
        urls = url_pattern.findall(' '.join(lines))
        return urls

    except FileNotFoundError:
        print("File not found.")
        return None


def start():
    urls = get_url()
    scrape_urls(urls, 'detailed_vulnerabilities.md')
